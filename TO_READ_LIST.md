- [NLP article] Understanding LSTM Networks [link](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [NLP article] The Unreasonable Effectiveness of Recurrent Neural Networks [link](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [NLP paper] Attention Is All You Need [link](https://arxiv.org/abs/1706.03762?context=cs)
- [NLP article] The Annotated Transformer [link](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [NLP paper] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [link](https://arxiv.org/abs/1810.04805)
- [NLP paper] RoBERTa: A Robustly Optimized BERT Pretraining Approach [link](https://arxiv.org/abs/1907.11692)
- [NLP paper] Beyond Accuracy: Behavioral Testing of NLP Models with CheckList [link](https://aclanthology.org/2020.acl-main.442/)
- [NLP paper] Character-level Convolutional Networks for Text Classification [link](https://papers.nips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html)
- [NLP paper] Language Models are Unsupervised Multitask Learners [link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [NLP paper] (T5) Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [link](https://arxiv.org/abs/1910.10683)
- [NLP paper] (GPT) Improving Language Understanding by Generative Pre-Training [link](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [NLP paper] (GPT-2) Language Models are Unsupervised Multitask Learners [link](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [NLP paper] (GPT-3) Language Models are Few-Shot Learners [link](https://arxiv.org/pdf/2005.14165.pdf)
